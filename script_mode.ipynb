{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "18Na-ThQ_05TRUEwCDiRwjelgDRaqcuum",
      "authorship_tag": "ABX9TyMN7W8axmurGRCtCXrdjfvo"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* data_setup.py (create dataset and dataloader)\n",
        "* model_builder.py (build model here)\n",
        "* engine.py (train_step(), test_step(), train() all are here)\n",
        "* utils.py (here save _model() function)\n",
        "* train.py (this will allow to all the file and train model with a single line command)\n"
      ],
      "metadata": {
        "id": "p72cDvTLfw1Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1ppqwSNCdAcM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"going_modular\", exist_ok = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/get_data.py\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it... \n",
        "if image_path.is_dir():\n",
        "    print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {image_path} directory, creating one...\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "# Download pizza, steak, sushi data\n",
        "with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "    print(\"Downloading pizza, steak, sushi data...\")\n",
        "    f.write(request.content)\n",
        "\n",
        "# Unzip pizza, steak, sushi data\n",
        "with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "    print(\"Unzipping pizza, steak, sushi data...\") \n",
        "    zip_ref.extractall(image_path)\n",
        "\n",
        "# Remove zip file\n",
        "os.remove(data_path / \"pizza_steak_sushi.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3r_ahr-jHOu",
        "outputId": "83a6a132-6f1b-4242-a26f-f1e04391af17"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/get_data.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python going_modular/get_data.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHsLfdXTxjQ0",
        "outputId": "b34706ff-cfa1-4bc0-d582-d58dcea6a957"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Did not find data/pizza_steak_sushi directory, creating one...\n",
            "Downloading pizza, steak, sushi data...\n",
            "Unzipping pizza, steak, sushi data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#turn above get data into a python script such as get_data.py\n",
        "#just use magic command"
      ],
      "metadata": {
        "id": "SWfDrIg3suwB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#to delete directory use following command\n",
        "!rm -rf data/"
      ],
      "metadata": {
        "id": "tOdXJH1Rua-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/data_setup.py\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "def create_dataloaders(\n",
        "  train_dir: str,\n",
        "  test_dir: str, \n",
        "  transform: transforms.Compose,\n",
        "  batch_size: int,\n",
        "  num_workers: int = NUM_WORKERS\n",
        "):\n",
        "  train_data = datasets.ImageFolder(train_dir, transform = transform)\n",
        "  test_data = datasets.ImageFolder(test_dir, transform = transform)\n",
        "  class_names = train_data.classes\n",
        "  train_dataloader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    num_workers = num_workers,\n",
        "    pin_memory=True,\n",
        "  )\n",
        "  test_dataloader = DataLoader(\n",
        "    test_data,\n",
        "    batch_size = batch_size,\n",
        "    shuffle =False,\n",
        "    num_workers = num_workers,\n",
        "    pin_memory = True,\n",
        "  )\n",
        "  return train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pw6mrgBldnwD",
        "outputId": "0a91faa1-7735-4d37-f618-eeea4d2e8f13"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/model_builder.py\n",
        "import torch\n",
        "from torch import nn\n",
        "class TinyVGG(nn.Module):\n",
        "  def __init__(self, input_shape:int, hidden_units: int, output_shape:int):\n",
        "    super().__init__()\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels = input_shape,\n",
        "                  out_channels = hidden_units,\n",
        "                  kernel_size = 3,\n",
        "                  stride =1,\n",
        "                  padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels = hidden_units,\n",
        "                  out_channels = hidden_units,\n",
        "                  kernel_size =3,\n",
        "                  stride=1,\n",
        "                  padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride=2)\n",
        "    )\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "        nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride=2)\n",
        "    )\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features = hidden_units *13*13,\n",
        "                  out_features = output_shape)\n",
        "    )\n",
        "  def forward(self, x: torch.Tensor):\n",
        "      #x = self.conv_block_1(x)\n",
        "      #x = self.conv_block_2(x)\n",
        "      #x = self.classifier(x)\n",
        "      return self.classifier(self.conv_block_2(self.conv_block_1(x)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-GnXsSkfoad",
        "outputId": "460dbf4c-4e26-47c6-98b4-61218f4f4820"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/model_builder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/engine.py\n",
        "from typing import Dict, List, Tuple\n",
        "import torch \n",
        "from tqdm.auto import tqdm\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device):\n",
        "  model.train()\n",
        "  train_loss, train_acc = 0, 0\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    y_pred = model(X)\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss.item()\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "    train_acc += (y_pred_class == y).sum().item() /len(y_pred)\n",
        "  train_loss = train_loss /len(dataloader)\n",
        "  train_acc = train_acc /len(dataloader)\n",
        "  return train_loss, train_acc\n",
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device: torch.device):\n",
        "  model.eval()\n",
        "  test_loss, test_acc = 0, 0\n",
        "  with torch.inference_mode():\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      test_pred_logits = model(X)\n",
        "      loss = loss_fn(test_pred_logits, y)\n",
        "      test_loss += loss.item()\n",
        "      test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "      test_acc += ((test_pred_labels ==y).sum().item() / len(test_pred_labels))\n",
        "  test_loss = test_loss / len(dataloader)\n",
        "  test_acc = test_acc /len(dataloader)\n",
        "  return test_loss, test_acc\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device):\n",
        "  results = {\"train_loss\": [],\n",
        "               \"train_acc\": [],\n",
        "               \"test_loss\": [],\n",
        "               \"test_acc\": []\n",
        "    }\n",
        "\n",
        "    # Loop through training and testing steps for a number of epochs\n",
        "  for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc = train_step(model=model,\n",
        "                                          dataloader=train_dataloader,\n",
        "                                          loss_fn=loss_fn,\n",
        "                                          optimizer=optimizer,\n",
        "                                          device=device)\n",
        "        test_loss, test_acc = test_step(model=model,\n",
        "          dataloader=test_dataloader,\n",
        "          loss_fn=loss_fn,\n",
        "          device=device)\n",
        "\n",
        "        # Print out what's happening\n",
        "        print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Update results dictionary\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "    # Return the filled results at the end of the epochs\n",
        "  return results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFVUQyo9jo-0",
        "outputId": "424c94c2-8dc0-4701-b02a-cc1cbaf17149"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/engine.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/utils.py\n",
        "from pathlib import Path\n",
        "import torch\n",
        "def save_model(model: torch.nn.Module,\n",
        "               target_dir: str,\n",
        "               model_name: str):\n",
        "  target_dir_path = Path(target_dir)\n",
        "  target_dir_path.mkdir(parents = True,\n",
        "                        exist_ok = True)\n",
        "  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
        "  model_save_path = target_dir_path /model_name\n",
        "  print(f\"[info] saving model to : {model_save_path}\")\n",
        "  torch.save(obj=model.state_dict(),\n",
        "             f =model_save_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qINNcQbFnelW",
        "outputId": "cfa3d1dc-f3d5-4f3b-9cbb-0ed3b831bcff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/train.py\n",
        "import os\n",
        "import torch \n",
        "from torchvision import transforms\n",
        "import data_setup, engine, model_builder, utils\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "HIDDEN_UNITS =10\n",
        "LEARNING_RATE = 0.001\n",
        "train_dir = \"data/pizza_steak_sushi/train\"\n",
        "test_dir = \"data/pizza_steak_sushi/test\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir = train_dir, \n",
        "    test_dir = test_dir,\n",
        "    transform = data_transform,\n",
        "    batch_size = BATCH_SIZE\n",
        ")\n",
        "model = model_builder.TinyVGG(\n",
        "    input_shape =3,\n",
        "    hidden_units = HIDDEN_UNITS,\n",
        "    output_shape = len(class_names)\n",
        ").to(device)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "engine.train(model = model,\n",
        "             train_dataloader = train_dataloader,\n",
        "             test_dataloader = test_dataloader,\n",
        "             loss_fn = loss_fn,\n",
        "             optimizer = optimizer,\n",
        "             epochs = NUM_EPOCHS,\n",
        "             device = device)\n",
        "utils.save_model(model=model,\n",
        "                 target_dir = \"models\",\n",
        "                 model_name = \"05_going_modular_script_mode_tinyvgg_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG8cwfoEoqhD",
        "outputId": "8ce7528b-cd7a-4a6b-c61e-2d68db2c5f70"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python going_modular/train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9sol6SBxVgg",
        "outputId": "0fb7c44c-eda1-4c7e-e34c-519dcaabb077"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  0% 0/5 [00:00<?, ?it/s]Epoch: 1 | train_loss: 1.1191 | train_acc: 0.2656 | test_loss: 1.1028 | test_acc: 0.2604\n",
            " 20% 1/5 [00:08<00:34,  8.61s/it]Epoch: 2 | train_loss: 1.1109 | train_acc: 0.3203 | test_loss: 1.1192 | test_acc: 0.1979\n",
            " 40% 2/5 [00:11<00:15,  5.00s/it]Epoch: 3 | train_loss: 1.0957 | train_acc: 0.2930 | test_loss: 1.0980 | test_acc: 0.1875\n",
            " 60% 3/5 [00:12<00:06,  3.44s/it]Epoch: 4 | train_loss: 1.0926 | train_acc: 0.5000 | test_loss: 1.0877 | test_acc: 0.6146\n",
            " 80% 4/5 [00:14<00:02,  2.63s/it]Epoch: 5 | train_loss: 1.0769 | train_acc: 0.5078 | test_loss: 1.0699 | test_acc: 0.5142\n",
            "100% 5/5 [00:15<00:00,  3.09s/it]\n",
            "[info] saving model to : models/05_going_modular_script_mode_tinyvgg_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now use argparse in train.py**\n",
        "\n",
        "use python argparse module to be able to send the train.py custom hyperparameter values for training procedures.\n",
        "like add an argument flag for using a different:\n",
        "* Training/testing directory\n",
        "* Learning rate\n",
        "* batch size\n",
        "* number of epochs to train for\n",
        "* number of hidden units in the TinyVGG model\n",
        "run like\n",
        "\n",
        "!python train.py --batch_size 64 --learning_rate 0.001 "
      ],
      "metadata": {
        "id": "HiPOeXePv0fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!python train.py --batch_size 64 --learning_rate 0.001 --num_epochs 25\n",
        "#modify train.py for accep extra argument"
      ],
      "metadata": {
        "id": "6jJ0B6PJxaPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/train_arg.py\n",
        "import os\n",
        "import argparse\n",
        "import torch \n",
        "from torchvision import transforms\n",
        "import data_setup, engine, model_builder, utils\n",
        "\n",
        "#create a parser\n",
        "parser = argparse.ArgumentParser(description = \"get some hyperparameters.\")\n",
        "\n",
        "#get an arg for num_epochs\n",
        "parser.add_argument(\"--num_epochs\",\n",
        "                    default =10,\n",
        "                    type=int,\n",
        "                    help=\"the number of epochs to train for\")\n",
        "#get an arg for batch_size\n",
        "parser.add_argument(\"--batch_size\",\n",
        "                    default = 32,\n",
        "                    type = int,\n",
        "                    help = \"the number of sample per batch \")\n",
        "#get an arg for hidden_units\n",
        "parser.add_argument(\"--hidden_units\",\n",
        "                    default=10,\n",
        "                    type=int,\n",
        "                    help = \" number of hidden units in hidden layers\")\n",
        "#get an arg for learning rate\n",
        "parser.add_argument(\"--learning_rate\",\n",
        "                    default = 0.001,\n",
        "                    type = float,\n",
        "                    help= \"learning rate to use for model\")\n",
        "\n",
        "#get out arguments from the parser\n",
        "args = parser.parse_args()\n",
        "#setup hyperparameters\n",
        "NUM_EPOCHS = args.num_epochs\n",
        "BATCH_SIZE = args.batch_size\n",
        "HIDDEN_UNITS =args.hidden_units\n",
        "LEARNING_RATE = args.learning_rate\n",
        "print(f\"[info] training a model for {NUM_EPOCHS} epochs with a {BATCH_SIZE} batch_size using {HIDDEN_UNITS} hidden unist and a learning rate {LEARNING_RATE} \")\n",
        "train_dir = \"data/pizza_steak_sushi/train\"\n",
        "test_dir = \"data/pizza_steak_sushi/test\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir = train_dir, \n",
        "    test_dir = test_dir,\n",
        "    transform = data_transform,\n",
        "    batch_size = BATCH_SIZE\n",
        ")\n",
        "model = model_builder.TinyVGG(\n",
        "    input_shape =3,\n",
        "    hidden_units = HIDDEN_UNITS,\n",
        "    output_shape = len(class_names)\n",
        ").to(device)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "engine.train(model = model,\n",
        "             train_dataloader = train_dataloader,\n",
        "             test_dataloader = test_dataloader,\n",
        "             loss_fn = loss_fn,\n",
        "             optimizer = optimizer,\n",
        "             epochs = NUM_EPOCHS,\n",
        "             device = device)\n",
        "utils.save_model(model=model,\n",
        "                 target_dir = \"models\",\n",
        "                 model_name = \"05_going_modular_script_mode_tinyvgg_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOAYujpH0CHB",
        "outputId": "8f8fb711-c1d2-4a69-9a72-f6f60d860517"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/train_arg.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python going_modular/train_arg.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55XGoHd56hJM",
        "outputId": "1b01b5fd-90a4-4eab-cf7b-daa76c1b2c07"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[info] training a model for 10 epochs with a 32 batch_size using 10 hidden unist and a learning rate 0.001 \n",
            "  0% 0/10 [00:00<?, ?it/s]Epoch: 1 | train_loss: 1.0971 | train_acc: 0.4336 | test_loss: 1.1054 | test_acc: 0.1979\n",
            " 10% 1/10 [00:04<00:37,  4.20s/it]Epoch: 2 | train_loss: 1.0862 | train_acc: 0.4141 | test_loss: 1.1118 | test_acc: 0.1979\n",
            " 20% 2/10 [00:05<00:20,  2.54s/it]Epoch: 3 | train_loss: 1.0885 | train_acc: 0.2930 | test_loss: 1.1134 | test_acc: 0.2292\n",
            " 30% 3/10 [00:06<00:14,  2.01s/it]Epoch: 4 | train_loss: 1.0558 | train_acc: 0.4453 | test_loss: 1.0601 | test_acc: 0.4044\n",
            " 40% 4/10 [00:08<00:10,  1.76s/it]Epoch: 5 | train_loss: 0.9385 | train_acc: 0.5977 | test_loss: 1.0321 | test_acc: 0.3210\n",
            " 50% 5/10 [00:09<00:08,  1.67s/it]Epoch: 6 | train_loss: 0.9987 | train_acc: 0.4922 | test_loss: 1.0274 | test_acc: 0.3627\n",
            " 60% 6/10 [00:11<00:06,  1.65s/it]Epoch: 7 | train_loss: 0.9169 | train_acc: 0.5781 | test_loss: 0.9627 | test_acc: 0.4536\n",
            " 70% 7/10 [00:12<00:04,  1.56s/it]Epoch: 8 | train_loss: 0.8848 | train_acc: 0.6328 | test_loss: 1.0642 | test_acc: 0.4044\n",
            " 80% 8/10 [00:14<00:03,  1.52s/it]Epoch: 9 | train_loss: 0.8125 | train_acc: 0.6602 | test_loss: 1.0506 | test_acc: 0.3722\n",
            " 90% 9/10 [00:16<00:01,  1.83s/it]Epoch: 10 | train_loss: 0.9251 | train_acc: 0.4648 | test_loss: 1.0178 | test_acc: 0.4441\n",
            "100% 10/10 [00:18<00:00,  1.87s/it]\n",
            "[info] saving model to : models/05_going_modular_script_mode_tinyvgg_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python going_modular/train_arg.py --num_epochs 3 --batch_size 64 --hidden_units 64 --learning_rate 0.002"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Duyy4VFR6pcT",
        "outputId": "c5acbd5e-573d-4b6b-b8f9-9997f9c6fef2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[info] training a model for 3 epochs with a 64 batch_size using 64 hidden unist and a learning rate 0.002 \n",
            "  0% 0/3 [00:00<?, ?it/s]Epoch: 1 | train_loss: 1.1786 | train_acc: 0.3292 | test_loss: 1.1011 | test_acc: 0.2862\n",
            " 33% 1/3 [00:03<00:06,  3.50s/it]Epoch: 2 | train_loss: 1.1005 | train_acc: 0.3216 | test_loss: 1.1755 | test_acc: 0.1953\n",
            " 67% 2/3 [00:06<00:02,  2.94s/it]Epoch: 3 | train_loss: 1.0717 | train_acc: 0.3958 | test_loss: 0.9941 | test_acc: 0.6953\n",
            "100% 3/3 [00:07<00:00,  2.51s/it]\n",
            "[info] saving model to : models/05_going_modular_script_mode_tinyvgg_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create an one argument for train directory"
      ],
      "metadata": {
        "id": "ipUbua5H7JPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/train_arg.py\n",
        "import os\n",
        "import argparse\n",
        "import torch \n",
        "from torchvision import transforms\n",
        "import data_setup, engine, model_builder, utils\n",
        "\n",
        "#create a parser\n",
        "parser = argparse.ArgumentParser(description = \"get some hyperparameters.\")\n",
        "\n",
        "#get an arg for num_epochs\n",
        "parser.add_argument(\"--num_epochs\",\n",
        "                    default =10,\n",
        "                    type=int,\n",
        "                    help=\"the number of epochs to train for\")\n",
        "#get an arg for batch_size\n",
        "parser.add_argument(\"--batch_size\",\n",
        "                    default = 32,\n",
        "                    type = int,\n",
        "                    help = \"the number of sample per batch \")\n",
        "#get an arg for hidden_units\n",
        "parser.add_argument(\"--hidden_units\",\n",
        "                    default=10,\n",
        "                    type=int,\n",
        "                    help = \" number of hidden units in hidden layers\")\n",
        "#get an arg for learning rate\n",
        "parser.add_argument(\"--learning_rate\",\n",
        "                    default = 0.001,\n",
        "                    type = float,\n",
        "                    help= \"learning rate to use for model\")\n",
        "\n",
        "#create an arg for training directory\n",
        "parser.add_argument(\"--train_dir\",\n",
        "                    default = \"data/pizza_steak_sushi/train\",\n",
        "                    type = str,\n",
        "                    help = \"directory file path to training data in standard image classification format\")\n",
        "\n",
        "#create an arg for test directory\n",
        "parser.add_argument(\"--test_dir\",\n",
        "                    default = \"data/pizza_steak_sushi/test\",\n",
        "                    type = str,\n",
        "                    help = \"directory file path to test data in standard image classificat\")\n",
        "#get out arguments from the parser\n",
        "args = parser.parse_args()\n",
        "#setup hyperparameters\n",
        "NUM_EPOCHS = args.num_epochs\n",
        "BATCH_SIZE = args.batch_size\n",
        "HIDDEN_UNITS =args.hidden_units\n",
        "LEARNING_RATE = args.learning_rate\n",
        "print(f\"[info] training a model for {NUM_EPOCHS} epochs with a {BATCH_SIZE} batch_size using {HIDDEN_UNITS} hidden unist and a learning rate {LEARNING_RATE} \")\n",
        "train_dir = args.train_dir\n",
        "test_dir = args.test_dir\n",
        "print(f\"[info] training data file: {train_dir}\")\n",
        "print(f\"[info] testing data file: {test_dir}\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((64,64)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir = train_dir, \n",
        "    test_dir = test_dir,\n",
        "    transform = data_transform,\n",
        "    batch_size = BATCH_SIZE\n",
        ")\n",
        "model = model_builder.TinyVGG(\n",
        "    input_shape =3,\n",
        "    hidden_units = HIDDEN_UNITS,\n",
        "    output_shape = len(class_names)\n",
        ").to(device)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
        "engine.train(model = model,\n",
        "             train_dataloader = train_dataloader,\n",
        "             test_dataloader = test_dataloader,\n",
        "             loss_fn = loss_fn,\n",
        "             optimizer = optimizer,\n",
        "             epochs = NUM_EPOCHS,\n",
        "             device = device)\n",
        "utils.save_model(model=model,\n",
        "                 target_dir = \"models\",\n",
        "                 model_name = \"05_going_modular_script_mode_tinyvgg_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcBI6nqE8S4L",
        "outputId": "8008100b-155f-4475-a948-5c9a6061c24f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting going_modular/train_arg.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python going_modular/train_arg.py --num_epochs 5 --batch_size 128 --hidden_units 128 --learning_rate 0.0003"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDWBiyKN-ERb",
        "outputId": "20fd742c-299e-4b8c-a466-c97216464722"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[info] training a model for 5 epochs with a 128 batch_size using 128 hidden unist and a learning rate 0.0003 \n",
            "[info] training data file: data/pizza_steak_sushi/train\n",
            "[info] testing data file: data/pizza_steak_sushi/test\n",
            "  0% 0/5 [00:00<?, ?it/s]Epoch: 1 | train_loss: 1.1036 | train_acc: 0.2967 | test_loss: 1.0976 | test_acc: 0.3333\n",
            " 20% 1/5 [00:03<00:13,  3.27s/it]Epoch: 2 | train_loss: 1.0894 | train_acc: 0.3821 | test_loss: 1.0853 | test_acc: 0.4000\n",
            " 40% 2/5 [00:04<00:06,  2.32s/it]Epoch: 3 | train_loss: 1.0681 | train_acc: 0.4884 | test_loss: 1.0573 | test_acc: 0.4400\n",
            " 60% 3/5 [00:06<00:03,  2.00s/it]Epoch: 4 | train_loss: 1.0277 | train_acc: 0.5802 | test_loss: 1.0252 | test_acc: 0.4400\n",
            " 80% 4/5 [00:08<00:01,  1.86s/it]Epoch: 5 | train_loss: 0.9611 | train_acc: 0.5129 | test_loss: 0.9985 | test_acc: 0.3600\n",
            "100% 5/5 [00:10<00:00,  2.14s/it]\n",
            "[info] saving model to : models/05_going_modular_script_mode_tinyvgg_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a python script to predict (suchs as predict.py) on a target image given a file path with a saved model.**\n",
        "\n",
        "* run the command python predict.py some_image.jpeg and have a trained pytorch model predict on the image and return its prediction."
      ],
      "metadata": {
        "id": "505w14_A-grn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/predict.py\n",
        "import torch\n",
        "import torchvision\n",
        "import argparse\n",
        "import model_builder\n",
        "\n",
        "#creating a parser\n",
        "parser = argparse.ArgumentParser()\n",
        "#get an image path\n",
        "parser.add_argument(\"--image\",\n",
        "                    help=\"target image to predict on\")\n",
        "\n",
        "#get a model path\n",
        "parser.add_argument(\"--model_path\",\n",
        "                    default = \"models/05_going_modular_script_mode_tinyvgg_model.pth\",\n",
        "                    type = str,\n",
        "                    help = \"target model to use for prediction  filepath\")\n",
        "\n",
        "args = parser.parse_args()\n",
        "\n",
        "#setup class names\n",
        "class_names = [\"pizza\", \"steak\", \"sushi\"]\n",
        "#setup device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "#get the image path\n",
        "IMG_PATH = args.image\n",
        "print(f\"[info] predictiin on {IMG_PATH}\")\n",
        "\n",
        "#fucntion to load in the model\n",
        "def load_model(filepath = args.model_path):\n",
        "  #need to use same hyperparamerter as saved model\n",
        "  model = model_builder.TinyVGG(input_shape =3,\n",
        "                                hidden_units = 128,\n",
        "                                output_shape =3).to(device)\n",
        "  print(f\"[info] loading in model from : {filepath}\")\n",
        "  #load in the saved model state dictionary from file\n",
        "  model.load_state_dict(torch.load(filepath))\n",
        "  \n",
        "  return model\n",
        "\n",
        "#fuction to laod in model + predict on select image\n",
        "def predict_on_image(image_path = IMG_PATH, filepath=args.model_path):\n",
        "  #load the model\n",
        "  model = load_model(filepath)\n",
        "  #load in the image and turn it into torch.float32 (same type as model)\n",
        "  image = torchvision.io.read_image(str(IMG_PATH)).type(torch.float32)\n",
        "\n",
        "  #preprocess the image\n",
        "  image = image /255.\n",
        "  #resise as same size in the model\n",
        "  transform = torchvision.transforms.Resize((64,64))\n",
        "  image = transform(image)\n",
        "  image = image.unsqueeze(dim=0)\n",
        "\n",
        "  #predcit on image\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    image = image.to(device)\n",
        "    pred_logits = model(image)\n",
        "    pred_prob = torch.softmax(pred_logits, dim=1)\n",
        "    pred_label = torch.argmax(pred_prob, dim=1)\n",
        "    pred_label_class = class_names[pred_label]\n",
        "  print(f\"[info] pred class: {pred_label_class}, pred prob: {pred_prob.max():.3f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  predict_on_image()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0VFxvdSI2um",
        "outputId": "7dfcbf95-21d1-45f5-92c3-cee65a99f36d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/predict.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python going_modular/predict.py --image data/pizza_steak_sushi/test/sushi/1172255.jpg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAMg2LK8RpH2",
        "outputId": "167780ae-88d4-4b18-cde0-fd60477ba745"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[info] predictiin on data/pizza_steak_sushi/test/sushi/1172255.jpg\n",
            "[info] loading in model from : models/05_going_modular_script_mode_tinyvgg_model.pth\n",
            "[info] pred class: pizza, pred prob: 0.469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#target is\n",
        "!python predict.py some_image.jpeg"
      ],
      "metadata": {
        "id": "1jjJhOCZJbas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/content/data"
      ],
      "metadata": {
        "id": "rrNdXTyt7C8N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}